{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 从零实现知识蒸馏\n",
    "\n",
    "使用 Qwen2.5-0.5B 作为教师模型，从零实现 ~123M 参数的 Decoder-Only Transformer 学生模型，\n",
    "在 Wikipedia 中文子集上进行在线知识蒸馏训练。\n",
    "\n",
    "**运行环境**: Google Colab T4 GPU (15GB VRAM)\n",
    "\n",
    "**模块结构**:\n",
    "- `src/config.py` — 配置数据类\n",
    "- `src/model.py` — 模型架构（RMSNorm, RoPE, GQA, SwiGLU）\n",
    "- `src/data.py` — 数据加载与预处理\n",
    "- `src/trainer.py` — 蒸馏训练循环\n",
    "- `src/generate.py` — 文本生成推理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: 环境检查 & 依赖安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "PyTorch: 2.9.0+cu126\n",
      "CUDA 可用: True\n",
      "GPU: Tesla T4\n",
      "显存: 14.7 GB\n",
      "\n",
      "使用设备: cuda\n",
      "begin\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖（Colab 环境）\n",
    "# !pip install torch transformers datasets matplotlib\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA 可用: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"显存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\n使用设备: {DEVICE}\")\n",
    "print(\"begin11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: 挂载 Google Drive（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "检查点目录: /content/drive/MyDrive/distillation_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "# 挂载 Google Drive 用于持久化检查点（可选）\n",
    "# 如果在本地运行，可跳过此 Cell\n",
    "\n",
    "USE_DRIVE = True  # 设置为 True 启用 Google Drive\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    CHECKPOINT_DIR = '/content/drive/MyDrive/distillation_checkpoints/'\n",
    "else:\n",
    "    CHECKPOINT_DIR = 'checkpoints/'\n",
    "\n",
    "import os\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "print(f\"检查点目录: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: 导入模块 & 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/LLM\n",
      "Already on 'stage1'\n",
      "Your branch is up to date with 'origin/stage1'.\n",
      "/content/LLM\n",
      "['.claude', 'models', '.specify', 'train_qwenold_colab.ipynb', 'tests', 'CLAUDE.md', '.qoder', 'notebooks', 'src', 'train_qwenold_colab.py', '__pycache__', 'requirements.txt', '.git', 'data', 'specs']\n",
      "模型配置: ModelConfig(hidden_size=512, num_layers=12, num_heads=8, num_kv_heads=2, intermediate_size=2048, vocab_size=151665, max_seq_len=512, rope_theta=1000000.0, norm_eps=1e-06, dropout=0.0)\n",
      "预期参数量: ~123,278,336\n",
      "\n",
      "训练配置: TrainingConfig(batch_size=8, learning_rate=0.0003, weight_decay=0.01, warmup_steps=500, num_epochs=3, gradient_clip=1.0, alpha=0.5, temperature=2.0, checkpoint_dir='/content/drive/MyDrive/distillation_checkpoints/', log_interval=50, eval_interval=500, save_interval=1000)\n"
     ]
    }
   ],
   "source": [
    "# 如果在 Colab 中，需要先克隆项目或上传 src/ 目录\n",
    "# !git clone <repo-url> && cd LLM\n",
    "\n",
    "import sys\n",
    "# !git clone https://github.com/linshengli/LLM.git\n",
    "%cd /content/LLM\n",
    "!git checkout stage1\n",
    "os.listdir('.')\n",
    "sys.path.insert(0, os.getcwd())  # 确保可以导入 src 模块\n",
    "print(os.getcwd())\n",
    "print(os.listdir('.'))\n",
    "\n",
    "from src.config import ModelConfig, TrainingConfig\n",
    "from src.model import StudentModel\n",
    "from src.data import load_tokenizer, create_dataloaders\n",
    "from src.trainer import DistillationTrainer, load_teacher_model\n",
    "from src.generate import TextGenerator, load_trained_model\n",
    "\n",
    "# 模型配置\n",
    "model_config = ModelConfig()\n",
    "print(f\"模型配置: {model_config}\")\n",
    "print(f\"预期参数量: ~{151665 * 512 + 3802112 * 12 + 512:,}\")\n",
    "\n",
    "# 训练配置\n",
    "training_config = TrainingConfig(checkpoint_dir=CHECKPOINT_DIR)\n",
    "print(f\"\\n训练配置: {training_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: 加载 Tokenizer & 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8e3c86de904f389c34d4677508eefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238a00de4bbb44a08a938449317abecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737a2a22cf404a1fba63e6d8d7fa7ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3386375ccf4edb9001bb67d88cb583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc47a79d5794d5f9b4733a210892327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab_size: 151643\n",
      "pad_token: <|endoftext|>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9483488fe3554619b21b988dd4e1a8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6664147e82b14353b7aeb9475b17bca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00000-of-00006.parquet:   0%|          | 0.00/587M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec0a0c82ef74c99bf4efc0e65f74f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00001-of-00006.parquet:   0%|          | 0.00/264M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5770a659233e47d4a2536db2c929f44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00002-of-00006.parquet:   0%|          | 0.00/127M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746b9edca6d14c8187d82bef1ee05984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00003-of-00006.parquet:   0%|          | 0.00/262M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3d7269e8b240ef9a613145d54e2db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00004-of-00006.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7284cbfd0134cb4a665669c374e673c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "20231101.zh/train-00005-of-00006.parquet:   0%|          | 0.00/225M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a08bc17a7434b64a474ee12003687b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1384748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "训练集 batch 数: 3305\n",
      "验证集 batch 数: 351\n",
      "\n",
      "样本 batch:\n",
      "  input_ids shape: torch.Size([8, 512])\n",
      "  labels shape: torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "# 加载 Tokenizer\n",
    "tokenizer = load_tokenizer()\n",
    "print(f\"Tokenizer vocab_size: {tokenizer.vocab_size}\")\n",
    "print(f\"pad_token: {tokenizer.pad_token}\")\n",
    "\n",
    "# 构建 DataLoader（取 5000 条文章，约 50MB 文本）\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    config=training_config,\n",
    "    model_config=model_config,\n",
    "    max_samples=5000,\n",
    ")\n",
    "\n",
    "print(f\"\\n训练集 batch 数: {len(train_loader)}\")\n",
    "print(f\"验证集 batch 数: {len(val_loader)}\")\n",
    "\n",
    "# 验证一个 batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\n样本 batch:\")\n",
    "print(f\"  input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "print(f\"  labels shape: {sample_batch['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: 构建学生模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学生模型参数量: 123,278,336 (123.3M)\n",
      "\n",
      "模型结构:\n",
      "StudentModel(\n",
      "  (embedding): Embedding(151665, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x TransformerBlock(\n",
      "      (attention_norm): RMSNorm()\n",
      "      (attention): GQAAttention(\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (o_proj): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (rotary_emb): RotaryEmbedding()\n",
      "      )\n",
      "      (ffn_norm): RMSNorm()\n",
      "      (ffn): SwiGLUFFN(\n",
      "        (gate_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "        (up_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "        (down_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (lm_head): Linear(in_features=512, out_features=151665, bias=False)\n",
      ")\n",
      "\n",
      "前向传播验证:\n",
      "  输入: torch.Size([2, 32])\n",
      "  输出: torch.Size([2, 32, 151665])\n",
      "  ✓ 输出形状正确\n",
      "\n",
      "显存占用:\n",
      "  已分配: 0.46 GB\n",
      "  已缓存: 0.48 GB\n"
     ]
    }
   ],
   "source": [
    "# 构建学生模型\n",
    "student_model = StudentModel(model_config)\n",
    "param_count = student_model.count_parameters()\n",
    "print(f\"学生模型参数量: {param_count:,} ({param_count / 1e6:.1f}M)\")\n",
    "print(f\"\\n模型结构:\")\n",
    "print(student_model)\n",
    "\n",
    "# 验证前向传播\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randint(0, model_config.vocab_size, (2, 32))\n",
    "    test_output = student_model(test_input)\n",
    "    print(f\"\\n前向传播验证:\")\n",
    "    print(f\"  输入: {test_input.shape}\")\n",
    "    print(f\"  输出: {test_output.shape}\")\n",
    "    assert test_output.shape == (2, 32, model_config.vocab_size)\n",
    "    print(\"  ✓ 输出形状正确\")\n",
    "\n",
    "# 显存状态\n",
    "if torch.cuda.is_available():\n",
    "    student_model = student_model.to(DEVICE)\n",
    "    print(f\"\\n显存占用:\")\n",
    "    print(f\"  已分配: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  已缓存: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: 加载教师模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30db82e260204ab8a4f01f1e70260003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "教师模型参数量: 494,032,768 (494.0M)\n",
      "\n",
      "显存占用（学生 + 教师）:\n",
      "  已分配: 1.39 GB\n",
      "  已缓存: 2.41 GB\n",
      "  剩余显存: 13.35 GB\n"
     ]
    }
   ],
   "source": [
    "# 加载教师模型（Qwen2.5-0.5B, FP16）\n",
    "teacher_model = load_teacher_model(device=DEVICE)\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "print(f\"教师模型参数量: {teacher_params:,} ({teacher_params / 1e6:.1f}M)\")\n",
    "\n",
    "# 显存状态\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n显存占用（学生 + 教师）:\")\n",
    "    print(f\"  已分配: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"  已缓存: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    free_mem = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()\n",
    "    print(f\"  剩余显存: {free_mem / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: 执行蒸馏训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始蒸馏训练...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.58 GiB is free. Process 15601 has 13.16 GiB memory in use. Of the allocated memory 12.79 GiB is allocated by PyTorch, and 253.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-657020378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 执行训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"开始蒸馏训练...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n训练完成！共 {len(history['train_loss'])} 步\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/LLM/src/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m# 计算蒸馏损失\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 loss, metrics = distillation_loss(\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \u001b[0mstudent_logits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mteacher_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 教师 FP16 → FP32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/LLM/src/trainer.py\u001b[0m in \u001b[0;36mdistillation_loss\u001b[0;34m(student_logits, teacher_logits, labels, alpha, temperature)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# KL 散度损失：比较温度缩放后的软化分布\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# log_softmax(student/T) 与 softmax(teacher/T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mstudent_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_logits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mteacher_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_logits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# KL(P || Q) = sum(P * (log(P) - log(Q)))，这里 P=teacher, Q=student\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2242\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.31 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.58 GiB is free. Process 15601 has 13.16 GiB memory in use. Of the allocated memory 12.79 GiB is allocated by PyTorch, and 253.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# 创建蒸馏训练器\n",
    "trainer = DistillationTrainer(\n",
    "    student_model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=training_config,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "# 执行训练\n",
    "print(\"开始蒸馏训练...\")\n",
    "history = trainer.train()\n",
    "print(f\"\\n训练完成！共 {len(history['train_loss'])} 步\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: 绘制 Loss 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 训练 Loss 曲线（使用滑动平均平滑）\n",
    "train_loss = history[\"train_loss\"]\n",
    "window = min(50, len(train_loss) // 5 + 1)\n",
    "if len(train_loss) > window:\n",
    "    smoothed = [sum(train_loss[max(0,i-window):i+1]) / len(train_loss[max(0,i-window):i+1])\n",
    "                for i in range(len(train_loss))]\n",
    "else:\n",
    "    smoothed = train_loss\n",
    "\n",
    "axes[0].plot(train_loss, alpha=0.3, label=\"原始\")\n",
    "axes[0].plot(smoothed, label=f\"滑动平均 (window={window})\")\n",
    "axes[0].set_xlabel(\"训练步数\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].set_title(\"训练 Loss 曲线\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 验证 Loss & PPL\n",
    "if history[\"val_loss\"]:\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history[\"val_loss\"], \"o-\", color=\"orange\", label=\"验证 Loss\")\n",
    "    ax2.set_xlabel(\"评估次数\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(\"验证集指标\")\n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(history[\"val_ppl\"], \"s--\", color=\"green\", label=\"PPL\")\n",
    "    ax2_twin.set_ylabel(\"Perplexity\")\n",
    "    ax2.legend(loc=\"upper left\")\n",
    "    ax2_twin.legend(loc=\"upper right\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"训练曲线已保存至 training_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: 文本生成演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 加载最佳模型\n",
    "best_ckpt = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "if not os.path.exists(best_ckpt):\n",
    "    best_ckpt = os.path.join(CHECKPOINT_DIR, \"final.pt\")\n",
    "\n",
    "gen_model = load_trained_model(best_ckpt, model_config, DEVICE)\n",
    "generator = TextGenerator(model=gen_model, tokenizer=tokenizer, device=DEVICE)\n",
    "\n",
    "# 测试提示词\n",
    "prompts = [\n",
    "    \"中国的首都是\",\n",
    "    \"人工智能的发展\",\n",
    "    \"数学是研究\",\n",
    "]\n",
    "\n",
    "strategies = [\"greedy\", \"top_k\", \"top_p\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"提示词: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for strategy in strategies:\n",
    "        result = generator.generate(\n",
    "            prompt,\n",
    "            max_new_tokens=100,\n",
    "            strategy=strategy,\n",
    "            temperature=0.8,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        print(f\"\\n[{strategy}]:\")\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: 保存最终模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 保存最终模型和配置\n",
    "final_save_dir = os.path.join(CHECKPOINT_DIR, \"final_model\")\n",
    "os.makedirs(final_save_dir, exist_ok=True)\n",
    "\n",
    "# 保存模型权重\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": gen_model.state_dict(),\n",
    "        \"model_config\": model_config,\n",
    "        \"training_config\": training_config,\n",
    "    },\n",
    "    os.path.join(final_save_dir, \"model.pt\"),\n",
    ")\n",
    "\n",
    "print(f\"模型已保存至: {final_save_dir}\")\n",
    "model_size = os.path.getsize(os.path.join(final_save_dir, \"model.pt\")) / 1024**2\n",
    "print(f\"模型文件大小: {model_size:.1f} MB\")\n",
    "print(f\"\\n训练摘要:\")\n",
    "print(f\"  学生模型参数量: {param_count:,}\")\n",
    "print(f\"  训练步数: {len(history['train_loss'])}\")\n",
    "if history['val_loss']:\n",
    "    print(f\"  最佳验证 Loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"  最佳验证 PPL: {min(history['val_ppl']):.2f}\")\n",
    "print(\"\\n蒸馏训练完成！\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
