# Tasks: PyTorch ä»é›¶å®ç° Qwen/DeepSeek æ¨¡å‹çŸ¥è¯†è’¸é¦

**Input**: Design documents from `/specs/001-pytorch-distillation/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/
**Scope**: Phase 1 only â€” User Story 1ï¼ˆä»é›¶æ„å»º Transformer æ¨¡å‹æ¶æ„ï¼‰

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: é¡¹ç›®åˆå§‹åŒ–å’ŒåŸºç¡€ç»“æ„æ­å»º

- [ ] T001 åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„ï¼š`src/`, `tests/`, `notebooks/`ï¼ŒæŒ‰ plan.md ä¸­çš„ Source Code å¸ƒå±€åˆ›å»ºæ‰€æœ‰ç›®å½•
- [ ] T002 åˆ›å»º `requirements.txt`ï¼ŒåŒ…å«ä¾èµ–ï¼štorch, transformers, datasets, pytest
- [ ] T003 [P] åˆ›å»º `src/__init__.py` å’Œ `tests/__init__.py`ï¼Œç¡®ä¿ Python åŒ…ç»“æ„æ­£ç¡®

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: é…ç½®æ•°æ®ç±»ï¼Œæ‰€æœ‰æ¨¡å—å…±äº«çš„åŸºç¡€è®¾æ–½

**âš ï¸ CRITICAL**: User Story 1 çš„æ¨¡å‹å®ç°ä¾èµ–æ­¤é˜¶æ®µçš„é…ç½®ç±»

- [ ] T004 [SETUP] å®ç° `ModelConfig` æ•°æ®ç±» in `src/config.py`
  - å­—æ®µï¼šhidden_size=512, num_layers=12, num_heads=8, num_kv_heads=2, intermediate_size=2048, vocab_size=151936, max_seq_len=512, rope_theta=1e6, norm_eps=1e-6, dropout=0.0
  - éªŒè¯è§„åˆ™ï¼šhidden_size % num_heads == 0, num_heads % num_kv_heads == 0
  - å‚è€ƒï¼šdata-model.md ModelConfig å®ä½“å®šä¹‰
- [ ] T005 [P] [SETUP] å®ç° `TrainingConfig` æ•°æ®ç±» in `src/config.py`
  - å­—æ®µï¼šbatch_size=8, learning_rate=3e-4, weight_decay=0.01, warmup_steps=500, num_epochs=3, gradient_clip=1.0, alpha=0.5, temperature=2.0, checkpoint_dir, log_interval=50, eval_interval=500, save_interval=1000
  - å‚è€ƒï¼šdata-model.md TrainingConfig å®ä½“å®šä¹‰

**Checkpoint**: é…ç½®åŸºç¡€å°±ç»ªï¼ŒUser Story 1 æ¨¡å‹å®ç°å¯ä»¥å¼€å§‹

---

## Phase 3: User Story 1 - ä»é›¶æ„å»º Transformer æ¨¡å‹æ¶æ„ (Priority: P1) ğŸ¯ MVP

**Goal**: ä»é›¶ä½¿ç”¨ PyTorch å®ç°ä¸€ä¸ª ~120M å‚æ•°çš„ Decoder-Only Transformer æ¨¡å‹ï¼Œå¯¹é½ Qwen2.5 æ¶æ„ç‰¹æ€§

**Independent Test**: æ„å»ºæ¨¡å‹ â†’ è¾“å…¥éšæœº token ID â†’ éªŒè¯è¾“å‡º logits å½¢çŠ¶ä¸º (batch, seq_len, vocab_size)ï¼Œå‚æ•°é‡çº¦ 123M

### Tests for User Story 1 âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [ ] T006 [P] [US1] ç¼–å†™é…ç½®éªŒè¯æµ‹è¯• in `tests/test_config.py`
  - æµ‹è¯• ModelConfig é»˜è®¤å€¼æ­£ç¡®æ€§
  - æµ‹è¯• hidden_size % num_heads != 0 æ—¶æŠ›å‡º ValueError
  - æµ‹è¯• num_heads % num_kv_heads != 0 æ—¶æŠ›å‡º ValueError
- [ ] T007 [P] [US1] ç¼–å†™æ¨¡å‹æ¶æ„æµ‹è¯• in `tests/test_model.py`
  - æµ‹è¯• RMSNorm: è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´ï¼Œå½’ä¸€åŒ–åå‡å€¼æ¥è¿‘ 0
  - æµ‹è¯• RotaryEmbedding: è¾“å‡ºå½¢çŠ¶ä¸å˜ï¼Œä¸åŒä½ç½®ç¼–ç ä¸åŒ
  - æµ‹è¯• GQAAttention: è¾“å…¥ (batch, seq, hidden) â†’ è¾“å‡ºå½¢çŠ¶ä¸€è‡´
  - æµ‹è¯• SwiGLUFFN: è¾“å…¥ (batch, seq, hidden) â†’ è¾“å‡ºå½¢çŠ¶ä¸€è‡´
  - æµ‹è¯• TransformerBlock: è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´ï¼Œæ®‹å·®è¿æ¥æœ‰æ•ˆ
  - æµ‹è¯• StudentModel: input_ids (batch, seq) â†’ logits (batch, seq, vocab_size)
  - æµ‹è¯• StudentModel.count_parameters() â‰ˆ 123M (Â±5%)
  - æµ‹è¯• lm_head ä¸ embedding æƒé‡å…±äº«ï¼ˆæ˜¯åŒä¸€ä¸ªå¼ é‡å¯¹è±¡ï¼‰

### Implementation for User Story 1

- [ ] T008 [P] [US1] å®ç° `RMSNorm` in `src/model.py`
  - Root Mean Square Layer Normalization
  - å‚æ•°ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾æƒé‡ gamma (hidden_size,)
  - å…¬å¼ï¼šx * rsqrt(mean(xÂ²) + eps) * gamma
  - å‚è€ƒï¼šcontracts/model.md RMSNorm æ¥å£
- [ ] T009 [P] [US1] å®ç° `RotaryEmbedding` in `src/model.py`
  - é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡çŸ©é˜µ freqs = 1 / (theta^(2i/dim))
  - æ ¹æ® position_ids ç”Ÿæˆ cos/sin ä½ç½®ç¼–ç 
  - å¯¹ Q/K å¼ é‡çš„å‰åŠ/ååŠç»´åº¦åº”ç”¨æ—‹è½¬å˜æ¢
  - å‚è€ƒï¼šcontracts/model.md RotaryEmbedding æ¥å£
- [ ] T010 [US1] å®ç° `GQAAttention` in `src/model.py`ï¼ˆdepends on T008, T009ï¼‰
  - Q æŠ•å½±: hidden_size â†’ num_heads * head_dim
  - K/V æŠ•å½±: hidden_size â†’ num_kv_heads * head_dim
  - KV å¤´æ‰©å±•ï¼ˆrepeat_kvï¼‰ï¼šå°† num_kv_heads æ‰©å±•åˆ° num_heads
  - ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› + å› æœé®ç½©ï¼ˆä¸Šä¸‰è§’ -inf maskï¼‰
  - å¯¹ Qã€K åº”ç”¨ RoPE
  - O æŠ•å½±: num_heads * head_dim â†’ hidden_size
  - å‚è€ƒï¼šcontracts/model.md GQAAttention æ¥å£
- [ ] T011 [P] [US1] å®ç° `SwiGLUFFN` in `src/model.py`
  - gate_proj: Linear(hidden_size, intermediate_size, bias=False)
  - up_proj: Linear(hidden_size, intermediate_size, bias=False)
  - down_proj: Linear(intermediate_size, hidden_size, bias=False)
  - å…¬å¼ï¼šdown_proj(SiLU(gate_proj(x)) * up_proj(x))
  - å‚è€ƒï¼šcontracts/model.md SwiGLUFFN æ¥å£
- [ ] T012 [US1] å®ç° `TransformerBlock` in `src/model.py`ï¼ˆdepends on T008, T010, T011ï¼‰
  - Pre-norm æ¶æ„ï¼šnorm â†’ attention â†’ residual â†’ norm â†’ ffn â†’ residual
  - attention_norm + attention + residual
  - ffn_norm + ffn + residual
  - å‚è€ƒï¼šcontracts/model.md TransformerBlock æ¥å£
- [ ] T013 [US1] å®ç° `StudentModel` in `src/model.py`ï¼ˆdepends on T012ï¼‰
  - embedding: nn.Embedding(vocab_size, hidden_size)
  - layers: nn.ModuleList of TransformerBlock Ã— num_layers
  - norm: æœ€ç»ˆ RMSNorm
  - lm_head: nn.Linear(hidden_size, vocab_size, bias=False)
  - **æƒé‡å…±äº«**: lm_head.weight = embedding.weight
  - è‡ªåŠ¨ç”Ÿæˆ position_ids å’Œå› æœ attention_mask
  - count_parameters() æ–¹æ³•
  - å‚è€ƒï¼šcontracts/model.md StudentModel æ¥å£
- [ ] T014 [US1] è¿è¡Œ `tests/test_model.py` éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡
  - å‰å‘ä¼ æ’­å½¢çŠ¶éªŒè¯
  - å‚æ•°é‡éªŒè¯ (~123M)
  - æƒé‡å…±äº«éªŒè¯
  - å„ç»„ä»¶ç‹¬ç«‹æµ‹è¯•

**Checkpoint**: User Story 1 å®Œæˆã€‚å­¦ç”Ÿæ¨¡å‹æ¶æ„ä»é›¶å®ç°ï¼Œèƒ½å¤Ÿæ¥å— token è¾“å…¥å¹¶è¾“å‡ºæ­£ç¡®å½¢çŠ¶çš„ logitsã€‚å¯ç‹¬ç«‹éªŒè¯ã€‚

---

## Dependencies & Execution Order

### Phase Dependencies

- **Setup (Phase 1)**: æ— ä¾èµ–ï¼Œå¯ç«‹å³å¼€å§‹
- **Foundational (Phase 2)**: ä¾èµ– Setup å®Œæˆï¼Œ**é˜»å¡** User Story 1
- **User Story 1 (Phase 3)**: ä¾èµ– Foundational å®Œæˆ

### Within User Story 1

```text
T006, T007 (æµ‹è¯•å…ˆè¡Œï¼Œå¯å¹¶è¡Œ)
    â†“
T008 (RMSNorm) â”€â”€â”
T009 (RoPE) â”€â”€â”€â”€â”€â”¤â”€â”€ å¯å¹¶è¡Œ
T011 (SwiGLUFFN) â”˜
    â†“
T010 (GQAAttention) â”€â”€ depends on T008, T009
    â†“
T012 (TransformerBlock) â”€â”€ depends on T008, T010, T011
    â†“
T013 (StudentModel) â”€â”€ depends on T012
    â†“
T014 (è¿è¡Œæµ‹è¯•éªŒè¯)
```

### Parallel Opportunities

- T006 & T007: æµ‹è¯•æ–‡ä»¶ä¸åŒï¼Œå¯å¹¶è¡Œç¼–å†™
- T008, T009, T011: ç‹¬ç«‹ç»„ä»¶ï¼Œä¸åŒç±»ï¼Œå¯å¹¶è¡Œå®ç°
- T004 & T005: åŒæ–‡ä»¶ä½†ä¸åŒç±»ï¼Œå¯å¹¶è¡Œï¼ˆæˆ–é¡ºåºå®ç°æ›´å®‰å…¨ï¼‰

---

## Notes

- æ‰€æœ‰ä»£ç éœ€é…å¤‡ä¸­æ–‡æ³¨é‡Šï¼ˆå®ªæ³•è¦æ±‚ï¼‰
- å…³é”®ç®—æ³•ï¼ˆRoPE æ—‹è½¬å˜æ¢ã€GQA å¤´æ‰©å±•ã€SwiGLU æ¿€æ´»ï¼‰éœ€è¡Œå†…æ³¨é‡Šè¯´æ˜æ•°å­¦åŸç†
- å…ˆå†™æµ‹è¯•å†å®ç°ï¼ˆTDDï¼Œå®ªæ³•å·¥ä½œæµç¨‹è¦æ±‚ï¼‰
- æ¯å®Œæˆä¸€ä¸ª Task åæäº¤ä¸€æ¬¡ commit
- Phase 2-4ï¼ˆæ•°æ®ã€è®­ç»ƒã€ç”Ÿæˆï¼‰å°†åœ¨åç»­è¿­ä»£ä¸­è§„åˆ’
