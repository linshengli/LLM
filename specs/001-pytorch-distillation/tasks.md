# Tasks: PyTorch ä»é›¶å®ç° Qwen/DeepSeek æ¨¡å‹çŸ¥è¯†è’¸é¦

**Input**: Design documents from `/specs/001-pytorch-distillation/`
**Prerequisites**: plan.md (required), spec.md (required for user stories), research.md, data-model.md, contracts/
**Scope**: å®Œæ•´ â€” US1ï¼ˆæ¨¡å‹æ¶æ„ï¼‰+ US2ï¼ˆæ•°æ®å‡†å¤‡ï¼‰+ US3ï¼ˆè’¸é¦è®­ç»ƒï¼‰+ US4ï¼ˆæ¨ç†ç”Ÿæˆï¼‰+ é›†æˆ Notebook

## Format: `[ID] [P?] [Story] Description`

- **[P]**: Can run in parallel (different files, no dependencies)
- **[Story]**: Which user story this task belongs to (e.g., US1, US2, US3)
- Include exact file paths in descriptions

---

## Phase 1: Setup (Shared Infrastructure)

**Purpose**: é¡¹ç›®åˆå§‹åŒ–å’ŒåŸºç¡€ç»“æ„æ­å»º

- [x] T001 åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„ï¼š`src/`, `tests/`, `notebooks/`ï¼ŒæŒ‰ plan.md ä¸­çš„ Source Code å¸ƒå±€åˆ›å»ºæ‰€æœ‰ç›®å½•
- [x] T002 åˆ›å»º `requirements.txt`ï¼ŒåŒ…å«ä¾èµ–ï¼štorch, transformers, datasets, pytest
- [x] T003 [P] åˆ›å»º `src/__init__.py` å’Œ `tests/__init__.py`ï¼Œç¡®ä¿ Python åŒ…ç»“æ„æ­£ç¡®

---

## Phase 2: Foundational (Blocking Prerequisites)

**Purpose**: é…ç½®æ•°æ®ç±»ï¼Œæ‰€æœ‰æ¨¡å—å…±äº«çš„åŸºç¡€è®¾æ–½

**âš ï¸ CRITICAL**: æ‰€æœ‰ User Story çš„å®ç°ä¾èµ–æ­¤é˜¶æ®µçš„é…ç½®ç±»

- [x] T004 [SETUP] å®ç° `ModelConfig` æ•°æ®ç±» in `src/config.py`
  - å­—æ®µï¼šhidden_size=512, num_layers=12, num_heads=8, num_kv_heads=2, intermediate_size=2048, vocab_size=151665, max_seq_len=512, rope_theta=1e6, norm_eps=1e-6, dropout=0.0
  - éªŒè¯è§„åˆ™ï¼šhidden_size % num_heads == 0, num_heads % num_kv_heads == 0
  - å‚è€ƒï¼šdata-model.md ModelConfig å®ä½“å®šä¹‰
  - **æ³¨æ„**: vocab_size ä»ç ”ç©¶é˜¶æ®µçš„ 151936 æ›´æ­£ä¸ºå®é™…å€¼ 151665ï¼ˆQwen2.5-0.5B Tokenizer å®æµ‹ï¼‰
- [x] T005 [P] [SETUP] å®ç° `TrainingConfig` æ•°æ®ç±» in `src/config.py`
  - å­—æ®µï¼šbatch_size=8, learning_rate=3e-4, weight_decay=0.01, warmup_steps=500, num_epochs=3, gradient_clip=1.0, alpha=0.5, temperature=2.0, checkpoint_dir, log_interval=50, eval_interval=500, save_interval=1000
  - å‚è€ƒï¼šdata-model.md TrainingConfig å®ä½“å®šä¹‰

**Checkpoint**: é…ç½®åŸºç¡€å°±ç»ªï¼Œæ‰€æœ‰ User Story å¯ä»¥å¼€å§‹

---

## Phase 3: User Story 1 - ä»é›¶æ„å»º Transformer æ¨¡å‹æ¶æ„ (Priority: P1) ğŸ¯ MVP

**Goal**: ä»é›¶ä½¿ç”¨ PyTorch å®ç°ä¸€ä¸ª ~120M å‚æ•°çš„ Decoder-Only Transformer æ¨¡å‹ï¼Œå¯¹é½ Qwen2.5 æ¶æ„ç‰¹æ€§

**Independent Test**: æ„å»ºæ¨¡å‹ â†’ è¾“å…¥éšæœº token ID â†’ éªŒè¯è¾“å‡º logits å½¢çŠ¶ä¸º (batch, seq_len, vocab_size)ï¼Œå‚æ•°é‡çº¦ 123M

### Tests for User Story 1 âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T006 [P] [US1] ç¼–å†™é…ç½®éªŒè¯æµ‹è¯• in `tests/test_config.py`
  - æµ‹è¯• ModelConfig é»˜è®¤å€¼æ­£ç¡®æ€§
  - æµ‹è¯• hidden_size % num_heads != 0 æ—¶æŠ›å‡º ValueError
  - æµ‹è¯• num_heads % num_kv_heads != 0 æ—¶æŠ›å‡º ValueError
- [x] T007 [P] [US1] ç¼–å†™æ¨¡å‹æ¶æ„æµ‹è¯• in `tests/test_model.py`
  - æµ‹è¯• RMSNorm: è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´ï¼Œå½’ä¸€åŒ–åå‡å€¼æ¥è¿‘ 0
  - æµ‹è¯• RotaryEmbedding: è¾“å‡ºå½¢çŠ¶ä¸å˜ï¼Œä¸åŒä½ç½®ç¼–ç ä¸åŒ
  - æµ‹è¯• GQAAttention: è¾“å…¥ (batch, seq, hidden) â†’ è¾“å‡ºå½¢çŠ¶ä¸€è‡´
  - æµ‹è¯• SwiGLUFFN: è¾“å…¥ (batch, seq, hidden) â†’ è¾“å‡ºå½¢çŠ¶ä¸€è‡´
  - æµ‹è¯• TransformerBlock: è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¸€è‡´ï¼Œæ®‹å·®è¿æ¥æœ‰æ•ˆ
  - æµ‹è¯• StudentModel: input_ids (batch, seq) â†’ logits (batch, seq, vocab_size)
  - æµ‹è¯• StudentModel.count_parameters() â‰ˆ 123M (Â±5%)
  - æµ‹è¯• lm_head ä¸ embedding æƒé‡å…±äº«ï¼ˆæ˜¯åŒä¸€ä¸ªå¼ é‡å¯¹è±¡ï¼‰

### Implementation for User Story 1

- [x] T008 [P] [US1] å®ç° `RMSNorm` in `src/model.py`
  - Root Mean Square Layer Normalization
  - å‚æ•°ï¼šå¯å­¦ä¹ çš„ç¼©æ”¾æƒé‡ gamma (hidden_size,)
  - å…¬å¼ï¼šx * rsqrt(mean(xÂ²) + eps) * gamma
  - å‚è€ƒï¼šcontracts/model.md RMSNorm æ¥å£
- [x] T009 [P] [US1] å®ç° `RotaryEmbedding` in `src/model.py`
  - é¢„è®¡ç®—æ—‹è½¬é¢‘ç‡çŸ©é˜µ freqs = 1 / (theta^(2i/dim))
  - æ ¹æ® position_ids ç”Ÿæˆ cos/sin ä½ç½®ç¼–ç 
  - å¯¹ Q/K å¼ é‡çš„å‰åŠ/ååŠç»´åº¦åº”ç”¨æ—‹è½¬å˜æ¢
  - å‚è€ƒï¼šcontracts/model.md RotaryEmbedding æ¥å£
- [x] T010 [US1] å®ç° `GQAAttention` in `src/model.py`ï¼ˆdepends on T008, T009ï¼‰
  - Q æŠ•å½±: hidden_size â†’ num_heads * head_dim
  - K/V æŠ•å½±: hidden_size â†’ num_kv_heads * head_dim
  - KV å¤´æ‰©å±•ï¼ˆrepeat_kvï¼‰ï¼šå°† num_kv_heads æ‰©å±•åˆ° num_heads
  - ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› + å› æœé®ç½©ï¼ˆä¸Šä¸‰è§’ -inf maskï¼‰
  - å¯¹ Qã€K åº”ç”¨ RoPE
  - O æŠ•å½±: num_heads * head_dim â†’ hidden_size
  - å‚è€ƒï¼šcontracts/model.md GQAAttention æ¥å£
- [x] T011 [P] [US1] å®ç° `SwiGLUFFN` in `src/model.py`
  - gate_proj: Linear(hidden_size, intermediate_size, bias=False)
  - up_proj: Linear(hidden_size, intermediate_size, bias=False)
  - down_proj: Linear(intermediate_size, hidden_size, bias=False)
  - å…¬å¼ï¼šdown_proj(SiLU(gate_proj(x)) * up_proj(x))
  - å‚è€ƒï¼šcontracts/model.md SwiGLUFFN æ¥å£
- [x] T012 [US1] å®ç° `TransformerBlock` in `src/model.py`ï¼ˆdepends on T008, T010, T011ï¼‰
  - Pre-norm æ¶æ„ï¼šnorm â†’ attention â†’ residual â†’ norm â†’ ffn â†’ residual
  - å‚è€ƒï¼šcontracts/model.md TransformerBlock æ¥å£
- [x] T013 [US1] å®ç° `StudentModel` in `src/model.py`ï¼ˆdepends on T012ï¼‰
  - embedding â†’ layers Ã— N â†’ final_norm â†’ lm_head
  - **æƒé‡å…±äº«**: lm_head.weight = embedding.weight
  - è‡ªåŠ¨ç”Ÿæˆ position_ids å’Œå› æœ attention_mask
  - count_parameters() æ–¹æ³•
  - å‚è€ƒï¼šcontracts/model.md StudentModel æ¥å£
- [x] T014 [US1] è¿è¡Œ `tests/test_model.py` å’Œ `tests/test_config.py` éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Checkpoint**: User Story 1 å®Œæˆã€‚å­¦ç”Ÿæ¨¡å‹æ¶æ„ä»é›¶å®ç°ï¼Œèƒ½å¤Ÿæ¥å— token è¾“å…¥å¹¶è¾“å‡ºæ­£ç¡®å½¢çŠ¶çš„ logitsã€‚

---

## Phase 4: User Story 2 - æ•°æ®å‡†å¤‡ä¸ Tokenizer é›†æˆ (Priority: P2)

**Goal**: åŠ è½½ Wikipedia ä¸­æ–‡å­é›†ï¼Œä½¿ç”¨ Qwen2.5 Tokenizer ç¼–ç ï¼Œæ„å»ºå› æœè¯­è¨€å»ºæ¨¡æ ¼å¼çš„ DataLoader

**Independent Test**: åŠ è½½æ•°æ® â†’ Tokenizer ç¼–ç /è§£ç  â†’ éªŒè¯ DataLoader è¾“å‡ºå½¢çŠ¶ä¸º (batch, seq_len)

### Tests for User Story 2 âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T015 [P] [US2] ç¼–å†™æ•°æ®ç®¡é“æµ‹è¯• in `tests/test_data.py`
  - æµ‹è¯• load_tokenizer è¿”å›æœ‰æ•ˆ Tokenizer ä¸” vocab_size=151936
  - æµ‹è¯• Tokenizer çš„ pad_token å·²è®¾ç½®
  - æµ‹è¯• WikiDataset.__len__() > 0
  - æµ‹è¯• WikiDataset.__getitem__() è¿”å› dict å« "input_ids" å’Œ "labels"
  - æµ‹è¯• input_ids shape = (seq_len,), dtype = torch.long
  - æµ‹è¯• labels[0] == -100ï¼ˆé¦–ä½å¿½ç•¥æ ‡è®°ï¼‰
  - æµ‹è¯• labels[1:] == input_ids[:-1] çš„å³ç§»å…³ç³»
  - æµ‹è¯• create_dataloaders è¿”å›ä¸¤ä¸ª DataLoader
  - æµ‹è¯• DataLoader è¾“å‡º batch shape = (batch_size, seq_len)

### Implementation for User Story 2

- [x] T016 [US2] å®ç° `load_tokenizer()` in `src/data.py`
  - ä½¿ç”¨ AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B")
  - è®¾ç½® pad_token = eos_tokenï¼ˆQwen2.5 é»˜è®¤æ—  pad_tokenï¼‰
  - å‚è€ƒï¼šcontracts/data.md load_tokenizer æ¥å£
- [x] T017 [US2] å®ç° `WikiDataset` in `src/data.py`ï¼ˆdepends on T016ï¼‰
  - ä» HuggingFace åŠ è½½ `wikipedia` æ•°æ®é›†ï¼Œconfig `20231101.zh`
  - æå– "text" å­—æ®µï¼Œä½¿ç”¨ tokenizer æ‰¹é‡ç¼–ç 
  - æ‹¼æ¥æ‰€æœ‰ token ä¸ºä¸€ç»´é•¿åºåˆ—ï¼ˆconcatenate-then-chunk ç­–ç•¥ï¼‰
  - æŒ‰ seq_len åˆ†å—ä¸ºç­‰é•¿æ ·æœ¬
  - æ„é€  labelsï¼šinput_ids å³ç§»ä¸€ä½ï¼Œé¦–ä½å¡«å…… -100
  - æ”¯æŒ max_samples å‚æ•°æ§åˆ¶æ•°æ®é‡ï¼ˆ~50MBï¼‰
  - æ”¯æŒ "train"/"validation" æ‹†åˆ†ï¼ˆ90/10ï¼‰
  - å‚è€ƒï¼šcontracts/data.md WikiDataset æ¥å£
- [x] T018 [US2] å®ç° `create_dataloaders()` in `src/data.py`ï¼ˆdepends on T017ï¼‰
  - åˆ›å»º train/val WikiDataset å®ä¾‹
  - æ„å»º DataLoaderï¼šshuffle(train=True, val=False), num_workers=2, pin_memory=True
  - å‚è€ƒï¼šcontracts/data.md create_dataloaders æ¥å£
- [x] T019 [US2] è¿è¡Œ `tests/test_data.py` éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Checkpoint**: User Story 2 å®Œæˆã€‚æ•°æ®ç®¡é“å°±ç»ªï¼Œèƒ½è¾“å‡ºæ­£ç¡®æ ¼å¼çš„è®­ç»ƒ/éªŒè¯ batchã€‚

---

## Phase 5: User Story 3 - çŸ¥è¯†è’¸é¦è®­ç»ƒ (Priority: P3)

**Goal**: ä½¿ç”¨ Qwen2.5-0.5B æ•™å¸ˆæ¨¡å‹å¯¹å­¦ç”Ÿæ¨¡å‹è¿›è¡Œåœ¨çº¿çŸ¥è¯†è’¸é¦ï¼Œè®­ç»ƒ loss æŒç»­ä¸‹é™ï¼ŒéªŒè¯é›† perplexity ä¼˜äºåŸºçº¿

**Independent Test**: æ‰§è¡Œå°‘é‡ step è®­ç»ƒ â†’ éªŒè¯ loss ä¸‹é™ â†’ ä¿å­˜/åŠ è½½æ£€æŸ¥ç‚¹ â†’ éªŒè¯æ¢å¤æ­£ç¡®

### Tests for User Story 3 âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T020 [P] [US3] ç¼–å†™è®­ç»ƒæ¨¡å—æµ‹è¯• in `tests/test_trainer.py`
  - æµ‹è¯• distillation_loss è¿”å› (total_loss, metrics_dict)
  - æµ‹è¯• total_loss ä¸ºæ ‡é‡å¼ é‡ä¸” > 0
  - æµ‹è¯• metrics_dict åŒ…å« "kl_loss", "ce_loss", "total_loss"
  - æµ‹è¯• alpha=0 æ—¶ total_loss â‰ˆ ce_loss
  - æµ‹è¯• alpha=1 æ—¶ total_loss â‰ˆ TÂ²Â·kl_loss
  - æµ‹è¯• load_teacher_model è¿”å›å†»ç»“çš„æ¨¡å‹ï¼ˆrequires_grad=Falseï¼‰
  - æµ‹è¯• DistillationTrainer.save_checkpoint/load_checkpoint å¾€è¿”ä¸€è‡´
  - æµ‹è¯• DistillationTrainer è®­ç»ƒ 10 æ­¥å loss ä¸‹é™

### Implementation for User Story 3

- [x] T021 [US3] å®ç° `distillation_loss()` in `src/trainer.py`
  - KL æ•£åº¦ï¼šF.kl_div(log_softmax(student/T), softmax(teacher/T), reduction="batchmean")
  - äº¤å‰ç†µï¼šF.cross_entropy(student.view(-1, V), labels.view(-1), ignore_index=-100)
  - åŠ æƒç»„åˆï¼šÎ±Â·TÂ²Â·KL + (1-Î±)Â·CE
  - è¿”å› (total_loss, {"kl_loss": ..., "ce_loss": ..., "total_loss": ...})
  - å‚è€ƒï¼šcontracts/trainer.md distillation_loss æ¥å£
- [x] T022 [US3] å®ç° `load_teacher_model()` in `src/trainer.py`
  - AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)
  - model.eval()ï¼Œå†»ç»“æ‰€æœ‰å‚æ•° param.requires_grad_(False)
  - ç§»åŠ¨åˆ°æŒ‡å®š device
  - å‚è€ƒï¼šcontracts/trainer.md load_teacher_model æ¥å£
- [x] T023 [US3] å®ç° `DistillationTrainer.__init__()` in `src/trainer.py`ï¼ˆdepends on T021, T022ï¼‰
  - åˆå§‹åŒ– AdamW ä¼˜åŒ–å™¨ï¼ˆlr, weight_decayï¼‰
  - åˆå§‹åŒ– cosine scheduler with linear warmup
  - å†»ç»“æ•™å¸ˆæ¨¡å‹ï¼Œè®¾ç½® eval æ¨¡å¼
  - åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€å˜é‡ï¼ˆepoch, step, best_val_lossï¼‰
  - å‚è€ƒï¼šcontracts/trainer.md DistillationTrainer æ¥å£
- [x] T024 [US3] å®ç° `DistillationTrainer.train()` in `src/trainer.py`ï¼ˆdepends on T023ï¼‰
  - è®­ç»ƒå¾ªç¯ï¼šéå† epoch Ã— batches
  - æ¯æ­¥ï¼šæ•™å¸ˆå‰å‘(no_grad) â†’ å­¦ç”Ÿå‰å‘ â†’ è®¡ç®—è’¸é¦ loss â†’ åå‘ä¼ æ’­ â†’ æ¢¯åº¦è£å‰ª â†’ ä¼˜åŒ–å™¨ step â†’ scheduler step
  - æ¯ log_interval æ­¥æ‰“å° loss, lr, step
  - æ¯ eval_interval æ­¥è°ƒç”¨ evaluate()
  - æ¯ save_interval æ­¥è°ƒç”¨ save_checkpoint()
  - NaN/Inf æ¢¯åº¦æ£€æµ‹ï¼štorch.isnan/isinf æ£€æŸ¥ï¼Œè®°å½•è­¦å‘Š
  - è®­ç»ƒç»“æŸä¿å­˜æœ€ç»ˆæ£€æŸ¥ç‚¹
  - è¿”å›è®­ç»ƒå†å²å­—å…¸
- [x] T025 [US3] å®ç° `DistillationTrainer.evaluate()` in `src/trainer.py`
  - å­¦ç”Ÿæ¨¡å‹ eval æ¨¡å¼éå†éªŒè¯é›†
  - è®¡ç®—å¹³å‡ val_loss å’Œ perplexity (exp(loss))
  - æ¢å¤å­¦ç”Ÿæ¨¡å‹ train æ¨¡å¼
  - è¿”å› {"val_loss": float, "val_ppl": float}
- [x] T026 [US3] å®ç° `save_checkpoint()` å’Œ `load_checkpoint()` in `src/trainer.py`
  - ä¿å­˜ï¼šstudent_model.state_dict(), optimizer, scheduler, epoch, step, best_val_loss, config
  - åŠ è½½ï¼šæ¢å¤æ‰€æœ‰çŠ¶æ€ï¼Œæ”¯æŒä»ä¸­æ–­å¤„ç»§ç»­è®­ç»ƒ
  - ä½¿ç”¨ torch.save/torch.load
- [x] T027 [US3] è¿è¡Œ `tests/test_trainer.py` éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Checkpoint**: User Story 3 å®Œæˆã€‚è’¸é¦è®­ç»ƒèƒ½åŠ›å°±ç»ªï¼Œå¯ç«¯åˆ°ç«¯è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹ã€‚

---

## Phase 6: User Story 4 - æ¨¡å‹æ¨ç†ä¸æ–‡æœ¬ç”Ÿæˆ (Priority: P4)

**Goal**: åŠ è½½è’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹ï¼Œæ”¯æŒè´ªå¿ƒ/top-k/top-p è§£ç ç­–ç•¥ç”Ÿæˆæ–‡æœ¬

**Independent Test**: åŠ è½½æ£€æŸ¥ç‚¹ â†’ è¾“å…¥æç¤ºè¯ â†’ ç”Ÿæˆæ–‡æœ¬ â†’ éªŒè¯éç©ºä¸”å¯è§£ç 

### Tests for User Story 4 âš ï¸

> **NOTE: Write these tests FIRST, ensure they FAIL before implementation**

- [x] T028 [P] [US4] ç¼–å†™ç”Ÿæˆæ¨¡å—æµ‹è¯• in `tests/test_generate.py`
  - æµ‹è¯• load_trained_model è¿”å› eval æ¨¡å¼çš„ StudentModel
  - æµ‹è¯• TextGenerator.generate(strategy="greedy") è¿”å›éç©ºå­—ç¬¦ä¸²
  - æµ‹è¯• TextGenerator.generate(strategy="top_k") è¿”å›éç©ºå­—ç¬¦ä¸²
  - æµ‹è¯• TextGenerator.generate(strategy="top_p") è¿”å›éç©ºå­—ç¬¦ä¸²
  - æµ‹è¯•ç”Ÿæˆæ–‡æœ¬é•¿åº¦ â‰¤ len(prompt_tokens) + max_new_tokens
  - æµ‹è¯•ä¸åŒ temperature ä¸‹ç”Ÿæˆç»“æœæœ‰å·®å¼‚ï¼ˆtemperature=0.1 vs 1.5ï¼‰

### Implementation for User Story 4

- [x] T029 [US4] å®ç° `load_trained_model()` in `src/generate.py`
  - åˆ›å»º StudentModel(config)
  - ä»æ£€æŸ¥ç‚¹åŠ è½½ model_state_dict
  - è®¾ç½® eval() æ¨¡å¼å¹¶ç§»åŠ¨åˆ° device
  - å‚è€ƒï¼šcontracts/generate.md load_trained_model æ¥å£
- [x] T030 [US4] å®ç° `TextGenerator.__init__()` in `src/generate.py`
  - ä¿å­˜ model, tokenizer, device å¼•ç”¨
  - ç¡®è®¤ model å¤„äº eval æ¨¡å¼
- [x] T031 [US4] å®ç° `TextGenerator.generate()` in `src/generate.py`ï¼ˆdepends on T030ï¼‰
  - ç¼–ç  prompt â†’ input_ids
  - è‡ªå›å½’ç”Ÿæˆå¾ªç¯ï¼š
    - å‰å‘ä¼ æ’­è·å– next_token_logits
    - æ ¹æ® strategy é€‰æ‹© next_tokenï¼š
      - greedy: argmax
      - top_k: ä¿ç•™å‰ k ä¸ª logitï¼Œsoftmax åé‡‡æ ·
      - top_p: æŒ‰æ¦‚ç‡é™åºç´¯ç§¯è‡³é˜ˆå€¼ pï¼Œåœ¨èŒƒå›´å†…é‡‡æ ·
    - temperature ç¼©æ”¾ï¼šlogits / temperature
    - æ‹¼æ¥ next_token åˆ°åºåˆ—
    - é‡åˆ° eos_token æˆ–è¾¾åˆ° max_new_tokens æ—¶åœæ­¢
  - è§£ç ç”Ÿæˆçš„ token åºåˆ—ä¸ºæ–‡æœ¬
  - å‚è€ƒï¼šcontracts/generate.md TextGenerator æ¥å£
- [x] T032 [US4] è¿è¡Œ `tests/test_generate.py` éªŒè¯æ‰€æœ‰æµ‹è¯•é€šè¿‡

**Checkpoint**: User Story 4 å®Œæˆã€‚æ¨ç†ç”Ÿæˆèƒ½åŠ›å°±ç»ªï¼Œå¯æ ¹æ®æç¤ºè¯ç”Ÿæˆæ–‡æœ¬ã€‚

---

## Phase 7: Integration & Notebook

**Purpose**: åˆ›å»º Colab ä¸» Notebookï¼Œæ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œç«¯åˆ°ç«¯æ¼”ç¤º

- [x] T033 [INTEG] åˆ›å»º `notebooks/main.ipynb` â€” Colab ä¸» Notebook
  - Cell 1: ç¯å¢ƒæ£€æŸ¥ & pip installï¼ˆtorch, transformers, datasetsï¼‰
  - Cell 2: æŒ‚è½½ Google Driveï¼ˆå¯é€‰ï¼Œç”¨äºæŒä¹…åŒ–æ£€æŸ¥ç‚¹ï¼‰
  - Cell 3: ä» src/ å¯¼å…¥æ‰€æœ‰æ¨¡å—ï¼Œæ‰“å°ç‰ˆæœ¬ä¿¡æ¯
  - Cell 4: åŠ è½½ Tokenizer & æ„å»º WikiDataset & DataLoader
  - Cell 5: æ„å»º StudentModel & æ‰“å°å‚æ•°é‡ & æ˜¾å­˜çŠ¶æ€
  - Cell 6: åŠ è½½æ•™å¸ˆæ¨¡å‹ & æ˜¾å­˜æ£€æŸ¥
  - Cell 7: åˆ›å»º DistillationTrainer & æ‰§è¡Œè®­ç»ƒ
  - Cell 8: ç»˜åˆ¶ loss æ›²çº¿ï¼ˆmatplotlibï¼‰
  - Cell 9: æ–‡æœ¬ç”Ÿæˆæ¼”ç¤ºï¼ˆgreedy / top-k / top-p å¯¹æ¯”ï¼‰
  - Cell 10: ä¿å­˜æœ€ç»ˆæ¨¡å‹

---

## Phase 8: Polish & Cross-Cutting Concerns

**Purpose**: ä»£ç è´¨é‡æ”¶å°¾

- [x] T034 [P] æ£€æŸ¥æ‰€æœ‰ .py æ–‡ä»¶ä¸­æ–‡æ³¨é‡Šå®Œæ•´æ€§ï¼ˆå®ªæ³•è¦æ±‚ï¼‰
- [x] T035 [P] è¿è¡Œå…¨é‡æµ‹è¯• `pytest tests/ -v` ç¡®è®¤é€šè¿‡
- [x] T036 éªŒè¯ quickstart.md æµç¨‹å¯å¤ç°ï¼ˆæœ¬åœ° + Colabï¼‰

---

## Dependencies & Execution Order

### Phase Dependencies

```text
Phase 1 (Setup)
    â†“
Phase 2 (Foundation: config.py)
    â†“
Phase 3 (US1: model.py) â”€â”€â†’ Phase 4 (US2: data.py) â”€â”€â†’ Phase 5 (US3: trainer.py) â”€â”€â†’ Phase 6 (US4: generate.py)
                                                                                            â†“
                                                                                     Phase 7 (Notebook)
                                                                                            â†“
                                                                                     Phase 8 (Polish)
```

### Within Each User Story

- æµ‹è¯•å…ˆè¡Œï¼ˆTDDï¼‰ï¼Œå…ˆå†™æµ‹è¯•ç¡®è®¤ FAIL
- æŒ‰ä¾èµ–é¡ºåºå®ç°å„ç»„ä»¶
- æœ€åè¿è¡Œæµ‹è¯•ç¡®è®¤å…¨éƒ¨ PASS

### Parallel Opportunities

- T006 & T007: ä¸åŒæµ‹è¯•æ–‡ä»¶ï¼Œå¯å¹¶è¡Œ
- T008, T009, T011: ç‹¬ç«‹æ¨¡å‹ç»„ä»¶ï¼Œå¯å¹¶è¡Œ
- T015, T020, T028: ä¸åŒæµ‹è¯•æ–‡ä»¶ï¼Œå¯å¹¶è¡Œç¼–å†™ï¼ˆä½†å®ç°éœ€æŒ‰ US é¡ºåºï¼‰
- T034, T035: ç‹¬ç«‹æ”¶å°¾ä»»åŠ¡ï¼Œå¯å¹¶è¡Œ

---

## Task Summary

| Phase | User Story | Tasks | æ–‡ä»¶ |
|-------|-----------|-------|------|
| 1 | Setup | T001-T003 | ç›®å½•, requirements.txt, __init__.py |
| 2 | Foundation | T004-T005 | src/config.py |
| 3 | US1 æ¨¡å‹æ¶æ„ | T006-T014 | src/model.py, tests/test_model.py, tests/test_config.py |
| 4 | US2 æ•°æ®å‡†å¤‡ | T015-T019 | src/data.py, tests/test_data.py |
| 5 | US3 è’¸é¦è®­ç»ƒ | T020-T027 | src/trainer.py, tests/test_trainer.py |
| 6 | US4 æ¨ç†ç”Ÿæˆ | T028-T032 | src/generate.py, tests/test_generate.py |
| 7 | Integration | T033 | notebooks/main.ipynb |
| 8 | Polish | T034-T036 | å…¨éƒ¨æ–‡ä»¶ |
| **Total** | | **36 tasks** | |

---

## Notes

- æ‰€æœ‰ä»£ç éœ€é…å¤‡ä¸­æ–‡æ³¨é‡Šï¼ˆå®ªæ³•è¦æ±‚ï¼‰
- å…³é”®ç®—æ³•ï¼ˆRoPEã€GQAã€SwiGLUã€KL è’¸é¦æŸå¤±ï¼‰éœ€è¡Œå†…æ³¨é‡Šè¯´æ˜æ•°å­¦åŸç†
- å…ˆå†™æµ‹è¯•å†å®ç°ï¼ˆTDDï¼Œå®ªæ³•å·¥ä½œæµç¨‹è¦æ±‚ï¼‰
- æ¯å®Œæˆä¸€ä¸ª Task åå»ºè®®æäº¤ä¸€æ¬¡ commit
- User Story é—´æŒ‰ P1â†’P2â†’P3â†’P4 é¡ºåºå®ç°ï¼Œå› ä¸ºå­˜åœ¨ä¾èµ–é“¾
