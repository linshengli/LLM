{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train qwenold.py (5-min Colab demo)\n",
        "\n",
        "This notebook trains `models/qwenold.py` on Tiny Shakespeare (character-level) for about 5 minutes and plots the training loss curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import urllib.request\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from models.qwenold import QwenModel\n",
        "\n",
        "print('torch', torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device', device)\n",
        "\n",
        "# Training budget (seconds)\n",
        "train_seconds = 300\n",
        "\n",
        "# Small defaults that should fit a free Colab GPU and finish quickly.\n",
        "block_size = 128\n",
        "batch_size = 64\n",
        "hidden_size = 128\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "\n",
        "lr = 3e-4\n",
        "weight_decay = 1e-2\n",
        "seed = 1337\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Logging\n",
        "log_every = 50\n",
        "eval_every = 200\n",
        "eval_iters = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Dataset (Tiny Shakespeare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TINY_SHAKESPEARE_URL = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "data_path = os.path.join('data', 'tinyshakespeare.txt')\n",
        "os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "\n",
        "if not (os.path.exists(data_path) and os.path.getsize(data_path) > 0):\n",
        "    print(f'Downloading dataset to {data_path} ...')\n",
        "    urllib.request.urlretrieve(TINY_SHAKESPEARE_URL, data_path)\n",
        "\n",
        "text = open(data_path, 'r', encoding='utf-8').read()\n",
        "print('chars:', len(text))\n",
        "\n",
        "# Trim to keep runtime consistent.\n",
        "text = text[:300_000]\n",
        "print('chars (trimmed):', len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Vocab + Encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class CharVocab:\n",
        "    stoi: Dict[str, int]\n",
        "    itos: List[str]\n",
        "\n",
        "    @property\n",
        "    def size(self) -> int:\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, s: str) -> List[int]:\n",
        "        return [self.stoi[c] for c in s]\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        return ''.join(self.itos[i] for i in ids)\n",
        "\n",
        "\n",
        "def build_vocab(text: str) -> CharVocab:\n",
        "    chars = sorted(set(text))\n",
        "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "    itos = chars\n",
        "    return CharVocab(stoi=stoi, itos=itos)\n",
        "\n",
        "\n",
        "def make_splits(text: str, train_frac: float = 0.9) -> Tuple[str, str]:\n",
        "    n = int(len(text) * train_frac)\n",
        "    return text[:n], text[n:]\n",
        "\n",
        "\n",
        "vocab = build_vocab(text)\n",
        "train_text, val_text = make_splits(text)\n",
        "train_ids = torch.tensor(vocab.encode(train_text), dtype=torch.long)\n",
        "val_ids = torch.tensor(vocab.encode(val_text), dtype=torch.long)\n",
        "\n",
        "print('vocab_size:', vocab.size)\n",
        "print('train tokens:', train_ids.numel(), 'val tokens:', val_ids.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers (Batching, Eval, Generation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(data: torch.Tensor, batch_size: int, block_size: int, device: torch.device):\n",
        "    ix = torch.randint(0, data.numel() - block_size - 1, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_data, val_data, batch_size, block_size, device, iters=20):\n",
        "    model.eval()\n",
        "    out = []\n",
        "    for data in (train_data, val_data):\n",
        "        losses = []\n",
        "        for _ in range(iters):\n",
        "            xb, yb = get_batch(data, batch_size, block_size, device)\n",
        "            logits = model(xb)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "            losses.append(loss.item())\n",
        "        out.append(sum(losses) / len(losses))\n",
        "    model.train()\n",
        "    return out[0], out[1]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, vocab: CharVocab, device: torch.device, prompt: str, max_new_tokens: int, block_size: int):\n",
        "    model.eval()\n",
        "    ids = torch.tensor([vocab.encode(prompt)], dtype=torch.long, device=device)\n",
        "    for _ in range(max_new_tokens):\n",
        "        ids_cond = ids[:, -block_size:]\n",
        "        logits = model(ids_cond)[:, -1, :]\n",
        "        probs = torch.softmax(logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        ids = torch.cat([ids, next_id], dim=1)\n",
        "    model.train()\n",
        "    return vocab.decode(ids[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = QwenModel(\n",
        "    vocab_size=vocab.size,\n",
        "    hidden_size=hidden_size,\n",
        "    max_seq_len=block_size,\n",
        "    num_layers=num_layers,\n",
        "    num_heads=num_heads,\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "use_amp = (device.type == 'cuda')\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "print('amp:', use_amp)\n",
        "\n",
        "print(\n",
        "    f'block={block_size} batch={batch_size} hidden={hidden_size} heads={num_heads} layers={num_layers} '\n",
        "    f'lr={lr} wd={weight_decay}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train (Time-Budgeted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_steps = []\n",
        "loss_values = []\n",
        "eval_steps = []\n",
        "eval_train_losses = []\n",
        "eval_val_losses = []\n",
        "\n",
        "start = time.time()\n",
        "step = 0\n",
        "model.train()\n",
        "\n",
        "while True:\n",
        "    if time.time() - start >= train_seconds:\n",
        "        break\n",
        "\n",
        "    xb, yb = get_batch(train_ids, batch_size, block_size, device)\n",
        "\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "        logits = model(xb)\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(opt)\n",
        "    scaler.update()\n",
        "\n",
        "    loss_steps.append(step)\n",
        "    loss_values.append(loss.item())\n",
        "\n",
        "    if step % log_every == 0:\n",
        "        elapsed = time.time() - start\n",
        "        print(f'step={step:5d} loss={loss.item():.4f} elapsed={elapsed:.1f}s')\n",
        "\n",
        "    if step > 0 and step % eval_every == 0:\n",
        "        tr, va = estimate_loss(model, train_ids, val_ids, batch_size, block_size, device, iters=eval_iters)\n",
        "        eval_steps.append(step)\n",
        "        eval_train_losses.append(tr)\n",
        "        eval_val_losses.append(va)\n",
        "        print(f'eval: train={tr:.4f} val={va:.4f}')\n",
        "\n",
        "    step += 1\n",
        "\n",
        "print('Training done. steps=', step, 'seconds=', round(time.time() - start, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(loss_steps, loss_values, label='train (per step)', alpha=0.7)\n",
        "if eval_steps:\n",
        "    plt.plot(eval_steps, eval_train_losses, label='train (eval)', linewidth=2)\n",
        "    plt.plot(eval_steps, eval_val_losses, label='val (eval)', linewidth=2)\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Training Loss')\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(generate(model, vocab, device, prompt='ROMEO:\\n', max_new_tokens=300, block_size=block_size))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

